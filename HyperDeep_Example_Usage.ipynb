{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HyperDeep_Example_Usage.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKy9nn9MCPW8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "c9714a80-fe0c-4733-afb3-5cb5771d8b9a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x83ibxsd7tao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "e8b12eb1-c702-448e-ab96-ff6bc9814576"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/6c/6608210b29649267de52001b09e369777ee2a5cfe1c71fa75eba82a4f2dc/catboost-0.24-cp36-none-manylinux1_x86_64.whl (65.9MB)\n",
            "\u001b[K     |████████████████████████████████| 65.9MB 53kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.0.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.18.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cD57x-A2mK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_validate\n",
        "from lightgbm import LGBMClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, RandomForestClassifier, RandomForestRegressor, AdaBoostClassifier, AdaBoostRegressor, ExtraTreesClassifier, ExtraTreesRegressor, BaggingRegressor, BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC, SVR\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "class Tuner:\n",
        "  def __init__(self, X_train, y_train, algo_type='binary', search_opt='randomized', scoring_fit='accuracy', n_iteration=100, cv=5):\n",
        "    self.X_train = X_train\n",
        "    self.y_train = y_train\n",
        "    self.algo_type = algo_type\n",
        "    self.search_opt = search_opt\n",
        "    self.scoring_fit = scoring_fit\n",
        "    self.n_iteration = n_iteration\n",
        "    self.cv = cv\n",
        "\n",
        "  def randomized_opt(self, model, parameter_grid):\n",
        "    randomized_search = RandomizedSearchCV(\n",
        "                        estimator=model,\n",
        "                        param_distributions=parameter_grid, \n",
        "                        cv=self.cv,\n",
        "                        n_iter=self.n_iteration,\n",
        "                        n_jobs=-1, \n",
        "                        scoring=self.scoring_fit,\n",
        "                        verbose=2)\n",
        "    fitted_model = randomized_search.fit(self.X_train, self.y_train)\n",
        "    return fitted_model\n",
        "\n",
        "  def grid_opt(self, model, parameter_grid):\n",
        "    grid_search = GridSearchCV(\n",
        "                        estimator=model,\n",
        "                        param_grid=parameter_grid, \n",
        "                        cv=self.cv,\n",
        "                        n_jobs=-1, \n",
        "                        scoring=self.scoring_fit,\n",
        "                        verbose=2)\n",
        "    fitted_model = grid_search.fit(self.X_train, self.y_train)\n",
        "    return fitted_model\n",
        "\n",
        "  def bayesian_opt(self, model, parameter_grid):\n",
        "    randomized_search = RandomizedSearchCV(\n",
        "                        estimator=model,\n",
        "                        param_distributions=parameter_grid, \n",
        "                        cv=self.cv,\n",
        "                        n_iter=self.n_iteration,\n",
        "                        n_jobs=-1, \n",
        "                        scoring=self.scoring_fit,\n",
        "                        verbose=2)\n",
        "    fitted_model = randomized_search.fit(self.X_train, self.y_train)\n",
        "    return fitted_model\n",
        "\n",
        "  def print_info(self, model):\n",
        "    print(model.best_score_)\n",
        "    print(model.best_params_)\n",
        "\n",
        "  def fit_LGBM(self):\n",
        "      \n",
        "    if self.algo_type == 'binary':\n",
        "      objective = LGBMClassifier(objective='binary')\n",
        "    elif self.algo_type == 'multiclass':\n",
        "      objective = LGBMClassifier(objective='multiclass')\n",
        "    elif self.algo_type =='regression':\n",
        "      objective = LGBMClassifier(objective='regression')\n",
        "      \n",
        "    print('Optimizing LGBM algorithm...')\n",
        "    param_grid = {\n",
        "        'num_leaves': [5, 10, 20, 30, 50, 100, 200],\n",
        "        'min_data_in_leaf': [10, 20, 30, 50, 100, 200, 500],\n",
        "        \"learning_rate\": [0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
        "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'min_child_weight': [0.001, 0.01, 0.1, 0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
        "        'reg_lambda': [0, 0.1, 1.0, 1.1, 1.5],\n",
        "        'reg_alpha': [0, 0.1, 1.0, 1.1, 1.5],\n",
        "        'n_estimators': [30, 70, 100, 150, 400, 700]\n",
        "    }\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n",
        "\n",
        "  \n",
        "  def fit_XGB(self):\n",
        "      \n",
        "    if self.algo_type == 'binary':\n",
        "      objective = xgb.XGBClassifier(objective='binary:logistic')\n",
        "    elif self.algo_type == 'multiclass':\n",
        "      objective = xgb.XGBClassifier(objective='multiclass:softmax')\n",
        "    elif self.algo_type =='regression':\n",
        "      objective = xgb.XGBClassifier(objective='regression:squarederror')\n",
        "\n",
        "    print('Optimizing XGB algorithm...')\n",
        "    param_grid = {\n",
        "        'max_depth': [None, 1, 3, 6, 10, 15, 20],\n",
        "        \"learning_rate\": [0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n",
        "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
        "        'gamma': [0, 0.25, 0.5, 1.0],\n",
        "        'reg_lambda': [0, 0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
        "        'n_estimators': [30, 70, 100, 150, 250, 400, 700]\n",
        "    }\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n",
        "\n",
        "  def fit_CatBoost(self):\n",
        "      \n",
        "    if self.algo_type == 'binary' or self.algo_type == 'multiclass':\n",
        "      objective = CatBoostClassifier()\n",
        "    elif self.algo_type =='regression':\n",
        "      objective = CatBoostRegressor()\n",
        "\n",
        "    print('Optimizing CatBoost algorithm...')\n",
        "\n",
        "    param_grid = {'depth':[3,1,2,6,4,5,7,8,9,10],\n",
        "              'iterations':[250,100,500,1000],\n",
        "              \"learning_rate\": [0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n",
        "              'l2_leaf_reg':[3,1,5,10,100],\n",
        "              'border_count':[32,5,10,20,50,100,200],\n",
        "              'ctr_border_count':[50,5,10,20,100,200],\n",
        "              }\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n",
        "\n",
        "  def fit_GBM(self):\n",
        "      \n",
        "    if self.algo_type == 'binary' or self.algo_type == 'multiclass':\n",
        "      objective = GradientBoostingClassifier()\n",
        "    elif self.algo_type =='regression':\n",
        "      objective = GradientBoostingRegressior()\n",
        "\n",
        "    print('Optimizing Gradient Boosting algorithm...')\n",
        "    param_grid = {\n",
        "        'max_depth': [None, 1, 3, 6, 10, 15, 20],\n",
        "        \"learning_rate\": [0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
        "        \"min_samples_split\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "        \"min_samples_leaf\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'n_estimators': [30, 70, 100, 150, 250, 400, 700],\n",
        "        \"max_features\":[\"auto\",\"log2\",\"sqrt\"],\n",
        "        \"criterion\": [\"friedman_mse\",  \"mae\", \"mse\"]\n",
        "    }\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n",
        "\n",
        "  def fit_RF(self):\n",
        "      \n",
        "    if self.algo_type == 'binary' or self.algo_type == 'multiclass':\n",
        "      objective = RandomForestClassifier()\n",
        "      param_grid = {\n",
        "          'max_depth': [None, 1, 3, 6, 10, 15, 20],\n",
        "          'bootstrap': [True, False],\n",
        "          \"min_samples_split\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          \"min_samples_leaf\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          'n_estimators': [30, 70, 100, 150, 250, 400, 700],\n",
        "          \"max_features\":[\"auto\",\"log2\",\"sqrt\"],\n",
        "          \"criterion\": [\"gini\", \"entropy\"]\n",
        "      }\n",
        "    elif self.algo_type =='regression':\n",
        "      objective = RandomForestRegressor()\n",
        "      param_grid = {\n",
        "          'max_depth': [None, 1, 3, 6, 10, 15, 20],\n",
        "          'bootstrap': [True, False],\n",
        "          \"min_samples_split\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          \"min_samples_leaf\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          'n_estimators': [30, 70, 100, 150, 250, 400, 700],\n",
        "          \"max_features\":[\"auto\",\"log2\",\"sqrt\"],\n",
        "          \"criterion\": [\"mse\", \"mae\"]\n",
        "      }\n",
        "    print('Optimizing Random Forest algorithm...')\n",
        "\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n",
        "\n",
        "  def fit_ExtraTrees(self):\n",
        "      \n",
        "    if self.algo_type == 'binary' or self.algo_type == 'multiclass':\n",
        "      objective = ExtraTreesClassifier()\n",
        "      param_grid = {\n",
        "          'max_depth': [None, 1, 3, 6, 10, 15, 20],\n",
        "          'bootstrap': [True, False],\n",
        "          \"min_samples_split\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          \"min_samples_leaf\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          'n_estimators': [30, 70, 100, 150, 250, 400, 700],\n",
        "          \"max_features\":[\"auto\",\"log2\",\"sqrt\"],\n",
        "          \"criterion\": [\"gini\", \"entropy\"]\n",
        "      }\n",
        "    elif self.algo_type =='regression':\n",
        "      objective = ExtraTreesRegressor()\n",
        "      param_grid = {\n",
        "          'max_depth': [None, 1, 3, 6, 10, 15, 20],\n",
        "          'bootstrap': [True, False],\n",
        "          \"min_samples_split\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          \"min_samples_leaf\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          'n_estimators': [30, 70, 100, 150, 250, 400, 700],\n",
        "          \"max_features\":[\"auto\",\"log2\",\"sqrt\"],\n",
        "          \"criterion\": [\"mae\", \"mse\"]\n",
        "      }\n",
        "\n",
        "    print('Optimizing ExtraTrees algorithm...')\n",
        "\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n",
        "\n",
        "  def fit_DT(self):\n",
        "      \n",
        "    if self.algo_type == 'binary' or self.algo_type == 'multiclass':\n",
        "      objective = DecisionTreeClassifier()\n",
        "      param_grid = {\n",
        "          'max_depth': [None, 1, 3, 6, 10, 15, 20],\n",
        "          \"min_samples_split\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          \"min_samples_leaf\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          \"max_features\":[\"auto\",\"log2\",\"sqrt\"],\n",
        "          \"criterion\": [\"gini\", \"entropy\"]\n",
        "      }\n",
        "    elif self.algo_type =='regression':\n",
        "      objective = DecisionTreeRegressor()\n",
        "      param_grid = {\n",
        "          'max_depth': [None, 1, 3, 6, 10, 15, 20],\n",
        "          \"min_samples_split\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          \"min_samples_leaf\": [0.1, 0.5, 1, 2, 3, 5, 10],\n",
        "          \"max_features\":[\"auto\",\"log2\",\"sqrt\"],\n",
        "          \"criterion\": [\"friedman_mse\",  \"mae\", \"mse\"]\n",
        "      }\n",
        "\n",
        "    print('Optimizing Decision Tree algorithm...')\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n",
        "\n",
        "  def fit_AdaBoost(self):\n",
        "      \n",
        "    if self.algo_type == 'binary' or self.algo_type == 'multiclass':\n",
        "      objective = AdaBoostClassifier()\n",
        "      param_grid = {\n",
        "                  'n_estimators': [30, 70, 100, 150, 250, 400, 700],\n",
        "                  \"learning_rate\": [0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30]\n",
        "                  }\n",
        "    elif self.algo_type =='regression':\n",
        "      objective = AdaBoostRegressor()\n",
        "      param_grid = {\n",
        "                  'n_estimators': [30, 70, 100, 150, 250, 400, 700],\n",
        "                  \"learning_rate\": [0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
        "                  'loss' : ['linear', 'square', 'exponential']\n",
        "                  }\n",
        "    print('Optimizing AdaBoost algorithm...')\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n",
        "\n",
        "  def fit_SVM(self):\n",
        "      \n",
        "    param_grid = {'C': [0.1, 1, 10, 30, 40, 100, 1000],\n",
        "                  'gamma': [1, 0.1, 0.01, 0.001],\n",
        "                  'kernel': ['rbf', 'poly', 'sigmoid']\n",
        "                  }\n",
        "    if self.algo_type == 'binary' or self.algo_type == 'multiclass':\n",
        "      objective = SVC()\n",
        "\n",
        "    elif self.algo_type =='regression':\n",
        "      objective = SVR()\n",
        "\n",
        "    print('Optimizing SVM algorithm...')\n",
        "\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n",
        "\n",
        "  def fit_LogR(self):\n",
        "\n",
        "    print('Optimizing Logistic Regression algorithm...')\n",
        "    param_grid = {\n",
        "        'penalty' : ['l1', 'l2'],\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "    }\n",
        "    objective = LogisticRegression()\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n",
        "\n",
        "  def fit_Bagging(self):\n",
        "\n",
        "    print('Optimizing Bagging algorithm...')\n",
        "    param_grid = {\n",
        "        'n_estimators': [30, 70, 100, 150, 250, 400, 700],\n",
        "        'max_samples': [0.05, 0.1, 0.2, 0.5, 1]\n",
        "    }\n",
        "    if self.algo_type == 'binary' or self.algo_type == 'multiclass':\n",
        "      objective = BaggingClassifier()\n",
        "    elif self.algo_type =='regression':\n",
        "      objective = BaggingRegressor()\n",
        "\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n",
        "\n",
        "  def fit_ANN(self):\n",
        "\n",
        "    param_grid = {\n",
        "                    'hidden_layer_sizes': [(3,), (6,), (10,), (15,), (20,), (30,), (40,), (50,), (100,), (150,), (200,),\n",
        "                                           (3,3,), (6,6,), (10,10,), (15,15,), (20,20,), (30,30,), (40,40,), (50,50,), (100,100,), (150,150,), (200,200),],\n",
        "                    'learning_rate_init': [0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
        "                    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "                    'batch_size': [25, 50, 100, 200, 300],\n",
        "                    'solver': ['lbfgs', 'sgd', 'adam'],\n",
        "                    'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
        "                    'alpha': [0, 0.001, 0.01, 0.1, 0.5, 1.0, 1.1, 1.5],\n",
        "                  }      \n",
        "\n",
        "    if self.algo_type == 'binary' or self.algo_type == 'multiclass':\n",
        "      objective = MLPClassifier(early_stopping=True)\n",
        "\n",
        "    elif self.algo_type =='regression':\n",
        "      objective = MLPRegressor(early_stopping=True)\n",
        "\n",
        "    print('Optimizing Artificial Neural Network...')\n",
        "\n",
        "\n",
        "    if self.search_opt == 'grid':\n",
        "      model = self.grid_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'bayesian':\n",
        "      model = self.bayesian_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'randomized':\n",
        "      model = self.randomized_opt(objective, param_grid)\n",
        "      self.print_info(model)\n",
        "      return model\n",
        "    elif self.search_opt == 'none':\n",
        "      scores = cross_validate(objective, self.X_train, self.y_train, cv=self.cv, return_estimator=True)\n",
        "      print(scores['test_score'].max())\n",
        "      return scores['estimator'][scores['test_score'].argmax()]\n",
        "    else:\n",
        "      raise ValueError('please provide a correct search optimization technique') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf1HQHBO6Six",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = pd.read_csv('/content/drive/Shared drives/celdum/X.csv')\n",
        "y = pd.read_csv('/content/drive/Shared drives/celdum/y.csv') \n",
        "del y['Unnamed: 0']\n",
        "del X['Unnamed: 0']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7_8YuvoDiDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tuner = Tuner(X_train,y_train, n_iteration=1500, search_opt='bayesian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjYhbplv-SDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "e277b022-250e-4cde-942d-67ef1ba12e08"
      },
      "source": [
        "LGBM = tuner.fit_LGBM()\n",
        "rfcpred = LGBM.predict(X_test)\n",
        "print(LGBM.best_params_)\n",
        "accuracy = accuracy_score(y_test, rfcpred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing LGBM algorithm...\n",
            "Fitting 5 folds for each of 1500 candidates, totalling 7500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   15.7s\n",
            "[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed:   42.1s\n",
            "[Parallel(n_jobs=-1)]: Done 365 tasks      | elapsed:  1.3min\n",
            "[Parallel(n_jobs=-1)]: Done 648 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done 1032 tasks      | elapsed:  3.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1562 tasks      | elapsed:  5.1min\n",
            "[Parallel(n_jobs=-1)]: Done 2119 tasks      | elapsed:  6.6min\n",
            "[Parallel(n_jobs=-1)]: Done 2854 tasks      | elapsed:  8.9min\n",
            "[Parallel(n_jobs=-1)]: Done 3595 tasks      | elapsed: 11.0min\n",
            "[Parallel(n_jobs=-1)]: Done 4484 tasks      | elapsed: 13.7min\n",
            "[Parallel(n_jobs=-1)]: Done 5423 tasks      | elapsed: 17.0min\n",
            "[Parallel(n_jobs=-1)]: Done 6454 tasks      | elapsed: 19.8min\n",
            "[Parallel(n_jobs=-1)]: Done 7500 out of 7500 | elapsed: 22.9min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9028639782080891\n",
            "{'subsample': 0.8, 'reg_lambda': 1.5, 'reg_alpha': 1.1, 'num_leaves': 100, 'n_estimators': 100, 'min_data_in_leaf': 10, 'min_child_weight': 0.001, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
            "{'subsample': 0.8, 'reg_lambda': 1.5, 'reg_alpha': 1.1, 'num_leaves': 100, 'n_estimators': 100, 'min_data_in_leaf': 10, 'min_child_weight': 0.001, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n",
            "0.8509346928866229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yx6q7EfF_wE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "e0182b9e-ff41-4f48-e96d-2607215f956c"
      },
      "source": [
        "XG = tuner.fit_XGB()\n",
        "rfcpred = XG.predict(X_test)\n",
        "print(XG.best_params_)\n",
        "accuracy = accuracy_score(y_test, rfcpred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing XGB algorithm...\n",
            "Fitting 5 folds for each of 1500 candidates, totalling 7500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   36.1s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  2.2min\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed:  5.1min\n",
            "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed:  8.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1009 tasks      | elapsed: 15.4min\n",
            "[Parallel(n_jobs=-1)]: Done 1454 tasks      | elapsed: 20.8min\n",
            "[Parallel(n_jobs=-1)]: Done 1991 tasks      | elapsed: 27.8min\n",
            "[Parallel(n_jobs=-1)]: Done 2598 tasks      | elapsed: 36.5min\n",
            "[Parallel(n_jobs=-1)]: Done 3287 tasks      | elapsed: 46.5min\n",
            "[Parallel(n_jobs=-1)]: Done 4056 tasks      | elapsed: 58.3min\n",
            "[Parallel(n_jobs=-1)]: Done 4939 tasks      | elapsed: 71.7min\n",
            "[Parallel(n_jobs=-1)]: Done 5874 tasks      | elapsed: 85.8min\n",
            "[Parallel(n_jobs=-1)]: Done 6887 tasks      | elapsed: 102.0min\n",
            "[Parallel(n_jobs=-1)]: Done 7500 out of 7500 | elapsed: 109.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.9014784153490851\n",
            "{'subsample': 0.7, 'silent': False, 'reg_lambda': 1.0, 'n_estimators': 30, 'min_child_weight': 3.0, 'max_depth': 20, 'learning_rate': 0.15, 'gamma': 1.0, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.8}\n",
            "{'subsample': 0.7, 'silent': False, 'reg_lambda': 1.0, 'n_estimators': 30, 'min_child_weight': 3.0, 'max_depth': 20, 'learning_rate': 0.15, 'gamma': 1.0, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.8}\n",
            "0.8543335761107065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbTbcJKR_w7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "8ce7266a-15b2-4c7f-f52b-e4c644e4b9f3"
      },
      "source": [
        "GB = tuner.fit_GBM()\n",
        "rfcpred = GB.predict(X_test)\n",
        "print(GB.best_params_)\n",
        "accuracy = accuracy_score(y_test, rfcpred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing Gradient Boosting algorithm...\n",
            "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  5.6min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 21.1min\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed: 31.8min\n",
            "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed: 74.4min\n",
            "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 111.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.899323147983656\n",
            "{'subsample': 0.6, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 6, 'learning_rate': 0.1, 'criterion': 'mae'}\n",
            "{'subsample': 0.6, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 6, 'learning_rate': 0.1, 'criterion': 'mae'}\n",
            "0.843084891154811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66fgeihU_7cQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "b8ff19b2-49a1-4de6-88f2-9cb85ae31133"
      },
      "source": [
        "RF = tuner.fit_RF()\n",
        "rfcpred = RF.predict(X_test)\n",
        "print(RF.best_params_)\n",
        "accuracy = accuracy_score(y_test, rfcpred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing Random Forest algorithm...\n",
            "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   22.8s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed:  3.3min\n",
            "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed:  5.7min\n",
            "[Parallel(n_jobs=-1)]: Done 1009 tasks      | elapsed:  8.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1454 tasks      | elapsed: 12.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1981 tasks      | elapsed: 17.2min\n",
            "[Parallel(n_jobs=-1)]: Done 2500 out of 2500 | elapsed: 20.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8976296559483627\n",
            "{'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2', 'max_depth': None, 'criterion': 'gini', 'bootstrap': False}\n",
            "{'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'log2', 'max_depth': None, 'criterion': 'gini', 'bootstrap': False}\n",
            "0.8507728413045238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-etQrj6_JtB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "0b14ddfc-d95c-4e68-ac65-36a4876c1ea3"
      },
      "source": [
        "AdaBoost = tuner.fit_AdaBoost()\n",
        "rfcpred = AdaBoost.predict(X_test)\n",
        "print(AdaBoost.best_params_)\n",
        "accuracy = accuracy_score(y_test, rfcpred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing AdaBoost algorithm...\n",
            "Fitting 5 folds for each of 49 candidates, totalling 245 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 49 is smaller than n_iter=1500. Running 49 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   35.1s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=-1)]: Done 245 out of 245 | elapsed:  4.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8865442056019421\n",
            "{'n_estimators': 700, 'learning_rate': 0.3}\n",
            "{'n_estimators': 700, 'learning_rate': 0.3}\n",
            "0.8621024520514688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0tlwnEo1ZaP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "77ab0929-7899-41e6-8780-6f838eac9920"
      },
      "source": [
        "ANN = tuner.fit_ANN()\n",
        "rfcpred = ANN.predict(X_test)\n",
        "print(ANN.best_params_)\n",
        "accuracy = accuracy_score(y_test, rfcpred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing Artificial Neural Network...\n",
            "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   40.5s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  3.7min\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed:  9.1min\n",
            "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed: 14.6min\n",
            "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 25.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8967053946823\n",
            "{'solver': 'sgd', 'learning_rate_init': 0.1, 'learning_rate': 'constant', 'hidden_layer_sizes': (200, 200), 'batch_size': 300, 'alpha': 1.5, 'activation': 'relu'}\n",
            "{'solver': 'sgd', 'learning_rate_init': 0.1, 'learning_rate': 'constant', 'hidden_layer_sizes': (200, 200), 'batch_size': 300, 'alpha': 1.5, 'activation': 'relu'}\n",
            "0.8339402767662054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLKzd7qb199v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "84281076-cb7d-4d64-ff53-d181a183ae96"
      },
      "source": [
        "SVM = tuner.fit_SVM()\n",
        "rfcpred = SVM.predict(X_test)\n",
        "print(SVM.best_params_)\n",
        "accuracy = accuracy_score(y_test, rfcpred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing SVM algorithm...\n",
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   27.0s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U--5DAA_qyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "0b737ed8-118b-4ac5-d753-05a083c6660e"
      },
      "source": [
        "Bagging = tuner.fit_Bagging()\n",
        "rfcpred = Bagging.predict(X_test)\n",
        "print(Bagging.best_params_)\n",
        "accuracy = accuracy_score(y_test, rfcpred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing Bagging algorithm...\n",
            "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 35 is smaller than n_iter=1500. Running 35 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   22.7s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  2.4min\n",
            "[Parallel(n_jobs=-1)]: Done 175 out of 175 | elapsed:  2.7min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.899169538698407\n",
            "{'n_estimators': 250, 'max_samples': 0.2}\n",
            "{'n_estimators': 250, 'max_samples': 0.2}\n",
            "0.8506919155134741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7soiiZpiA-Iq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "d9e2f43d-033e-4d6e-8d79-94f99eaa93e7"
      },
      "source": [
        "LOGR = tuner.fit_LogR()\n",
        "rfcpred = LOGR.predict(X_test)\n",
        "print(LOGR.best_params_)\n",
        "accuracy = accuracy_score(y_test, rfcpred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing Logistic Regression algorithm...\n",
            "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:281: UserWarning: The total space of parameters 14 is smaller than n_iter=1500. Running 14 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8806950908983241\n",
            "{'penalty': 'l2', 'C': 1}\n",
            "{'penalty': 'l2', 'C': 1}\n",
            "0.8483450675730355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    1.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93xEQhVuBSb1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "7c087d11-227e-41e2-97e1-5cf2993c8d44"
      },
      "source": [
        "DT = tuner.fit_DT()\n",
        "rfcpred = DT.predict(X_test)\n",
        "print(DT.best_params_)\n",
        "accuracy = accuracy_score(y_test, rfcpred)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimizing Decision Tree algorithm...\n",
            "Fitting 5 folds for each of 1500 candidates, totalling 7500 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done 358 tasks      | elapsed:    2.1s\n",
            "[Parallel(n_jobs=-1)]: Done 4230 tasks      | elapsed:   14.3s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8837759223071002\n",
            "{'min_samples_split': 3, 'min_samples_leaf': 10, 'max_features': 'auto', 'max_depth': 15, 'criterion': 'entropy'}\n",
            "{'min_samples_split': 3, 'min_samples_leaf': 10, 'max_features': 'auto', 'max_depth': 15, 'criterion': 'entropy'}\n",
            "0.8557902403495994\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done 7500 out of 7500 | elapsed:   24.6s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-6kYvvP4Qzo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "9450618e-2264-4f92-8036-8fea1a9d380a"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "print(classification_report(y_test, rfcpred, digits=4))\n",
        "print(roc_auc_score(y_test, rfcpred))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9892    0.8429    0.9102     10965\n",
            "           1     0.4283    0.9274    0.5860      1392\n",
            "\n",
            "    accuracy                         0.8524     12357\n",
            "   macro avg     0.7088    0.8852    0.7481     12357\n",
            "weighted avg     0.9260    0.8524    0.8737     12357\n",
            "\n",
            "0.8851530929131878\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}